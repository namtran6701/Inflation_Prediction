{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we apply machine learning methods to predict Consumer Price Index. \n",
    "\n",
    "After obtaining the predicted CPI, we would then calculate monthly and yearly inflation.\n",
    "\n",
    "After carefully considering the underlying structure of the data, we decided to build models using the period 2010-2020\n",
    "\n",
    "- 2010 - 2017 as training data\n",
    "\n",
    "- 2017 - 2019 as validation data\n",
    "\n",
    "- 2019 - 2020 as test data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Preprocessing "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Label Decomposition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "import seaborn as sns\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from statsmodels.tsa.arima.model import ARIMA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "\n",
    "df = pd.read_csv('cpi.csv', parse_dates= [['Year', 'Month']], index_col= 'Year_Month')\n",
    "\n",
    "# get data from 2010 to 2020\n",
    "df = df.loc['2010-01-01':'2019-12-31']\n",
    "\n",
    "# Set the monthly frequency for the data\n",
    "\n",
    "df.index.freq = 'MS'\n",
    "\n",
    "# Change the index name to 'Date'\n",
    "df.index.name = 'Date'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize monthly and yearly inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['1-Month % Change'].plot()\n",
    "plt.title('1-month inflation rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['12-Month % Change'].plot()\n",
    "plt.title('12-month inflation rate')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current main focus is the CPI index, so let's decompose this feature first.\n",
    "- First, decompose the CPI column into trend, seasonal, and residual components using additive method. \n",
    "\n",
    "- Second, apply multiplicative method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CPI'].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Additive decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additive_decomposed = seasonal_decompose(df['CPI'], \n",
    "                                         model='additive',\n",
    "                                         two_sided= False, \n",
    "                                         period= 6)\n",
    "\n",
    "# Plot the original data, trend, seasonal, and residual components\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "# Original data\n",
    "ax1.plot(df['CPI'])\n",
    "ax1.set_title('Original Data')\n",
    "ax1.grid()\n",
    "\n",
    "# Trend component\n",
    "ax2.plot(additive_decomposed.trend)\n",
    "ax2.set_title('Trend Component')\n",
    "ax2.grid()\n",
    "\n",
    "# Seasonal component\n",
    "ax3.plot(additive_decomposed.seasonal)\n",
    "ax3.set_title('Seasonal Component')\n",
    "ax3.grid()\n",
    "\n",
    "# Residual component\n",
    "ax4.plot(additive_decomposed.resid)\n",
    "ax4.set_title('Residual Component')\n",
    "ax4.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A statistical look into the seasonal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additive_decomposed.seasonal.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Multiplicative Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplicative_decomposed = seasonal_decompose(df['CPI'], \n",
    "                                               model='multiplicative',\n",
    "                                               two_sided= False, \n",
    "                                               period= 6)\n",
    "\n",
    "# Plot the original data, trend, seasonal, and residual components\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(10, 8), sharex=True)\n",
    "\n",
    "# Original data\n",
    "ax1.plot(df['CPI'])\n",
    "ax1.set_title('Original Data')\n",
    "ax1.grid()\n",
    "\n",
    "# Trend component\n",
    "ax2.plot(multiplicative_decomposed.trend)\n",
    "ax2.set_title('Trend Component')\n",
    "ax2.grid()\n",
    "\n",
    "# Seasonal component\n",
    "ax3.plot(multiplicative_decomposed.seasonal)\n",
    "ax3.set_title('Seasonal Component')\n",
    "ax3.grid()\n",
    "\n",
    "# Residual component\n",
    "ax4.plot(multiplicative_decomposed.resid)\n",
    "ax4.set_title('Residual Component')\n",
    "ax4.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Decomposition Conclusion\n",
    "\n",
    "- After trying multiple periods/frequencies, we decided to use a period of 6 to decompose the CPI index as it results the perfect seasonal component. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both multiplicative and additive decomposition show that the trend component is the most important component in the CPI index. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain statistical attributes of the trend component\n",
    "additive_decomposed.trend.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the series has a linear trend, it is definitely not stationary. Thus, we should attempt to make it stationary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we can address how statistical properties of a series change over time by visualizing. This would help us check the structural break and heteroscedasticity issue. \n",
    "- The rolling window size is 12 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fucntion to plot rolling variance and rolling mean\n",
    "def rolling_statistics(timeseries, custom_name, window_size=12):\n",
    "    # Determine rolling statistics\n",
    "    rolling_mean = timeseries.rolling(window=window_size).mean()\n",
    "    rolling_std = timeseries.rolling(window=window_size).std()\n",
    "\n",
    "    # Plot rolling statistics\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(rolling_mean, color='black', label='Rolling Mean')\n",
    "    plt.plot(rolling_std, color='red', label='Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('12 Periods Rolling Mean & Standard Deviation of ' + custom_name)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Label Diffencing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, attempt to difference the data to see if the process can make the data more stationary. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 First Order Differencing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first order differencing, we would subtract the immediate previous value from the current value to obtain the difference between two consecutive periods. \n",
    "\n",
    "First-Order Differencing = Value at time t - Value at time t-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_data = df['CPI'].diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_data.plot()\n",
    "plt.title('First - Order Differenced Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_statistics(diff_data, 'First - Order Differenced Data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Second Order Differencing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second-order difference is the difference of the differences. That is, it's the first-order difference of the first-order differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_order_diff = diff_data.diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_order_diff.plot()\n",
    "plt.title('Second - Order Differenced Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_statistics(second_order_diff, 'Second - Order Differenced Data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Label Detrending"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The method for smoothing data used in this project is backward moving average.\n",
    "\n",
    "- Detrended data is computed by subtracting the trend values from the actual values. \n",
    "\n",
    "- Since we use a period of 6 to smooth out the data, the function will use a backward moving average with a window size of 6 to smooth the trend component (6 periods prior to the current value).\n",
    "\n",
    "- As a result, we would lose 6 observations in using label detrending, compared to only 1 in first-order differencing, and 2 in second-order differencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, I extract the trend component from the multiplicative decomposition. Trend values from either multiplicative or additive decompositions are identical.\n",
    "trend = multiplicative_decomposed.trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detrend = df['CPI']- trend\n",
    "\n",
    "detrend.dropna(inplace=True)\n",
    "\n",
    "detrend.plot()\n",
    "\n",
    "plt.title('Detrended Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_statistics(detrend, 'Detrended Data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Differencing and Detrending Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean and variance of these transformed data are not constant over time. Among the 3 transformation methods, the second order differencing appear to be the most stationary. Therefore, we would move forward and investigate further the second order differencing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Transformed Label's Statistical Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a box plot to visualize the data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cus_boxplot(data1, title1):\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    sns.boxplot(data1, ax=ax1)\n",
    "    ax1.set_title(title1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cus_boxplot(second_order_diff, 'Second-Order Differenced Data')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the statistical description of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Second order difference data statistical summary:')\n",
    "second_order_diff.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stationarity and White Noise Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to calculate the ADF test and print out the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationary_test(input):\n",
    "\n",
    "    result = adfuller(input)\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n",
    "    print('Critical Values:', result[4])\n",
    "\n",
    "    # Reject the null hypothesis if the p-value is below the chosen significance level\n",
    "    if result[1] < 0.05:\n",
    "        print(\"The data is STATIONARY.\")\n",
    "    else:\n",
    "        print(\"The data is NOT STATIONARY.\")\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the **ADF** test, let's use the non parametric **KPSS** test to confirm the stationarity of the data. If KPSS's result contradict conclusion from ADF, we need to investigate further. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Augemnted Dickey-Fuller Test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To statistically verify if the data is stationary or not, we would deploy ADF test. \n",
    "\n",
    "- Null hypothesis: The time series contains a unit root and is non-stationary\n",
    "\n",
    "- Alternative hypothesis is that the time series is stationary. \n",
    "\n",
    "To confirm that the data is stationary, we need a p-value that is lower than the significance level in order to reject the null hypothesis, and the critical values should be greater greater than the ADF statistics.\n",
    "\n",
    "- The significance level chosen is 0.05. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADF on the second order differenced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationary_test(second_order_diff)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Non-parametric KPSS test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Null hypothesis: The time series is stationary (no unit root)\n",
    "\n",
    "- Alternative hypothesis: The time series is stastionary (it has a unit root)\n",
    "\n",
    "KPSS' test statistic is compared to the relevant critical values. If the test statistic is greater than the cirtical value at a chosen level of significance, we reject the null hypothesis  and conclude that the series is non-stationary with a unit root. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to perform the kpss test.\n",
    "def kpss_test(input):\n",
    "        result = kpss(input)\n",
    "        print('KPSS Statistic:', result[0])\n",
    "        print('p-value:', result[1])\n",
    "        print('Critical Values:', result[3])\n",
    "    \n",
    "        # Reject the null hypothesis if the p-value is below the chosen significance level\n",
    "        if result[1] < 0.05:\n",
    "            print(\"The data is NOT STATIONARY.\")\n",
    "        else:\n",
    "            print(\"The data is STATIONARY.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KPSS test on the second-order differenced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpss_test(second_order_diff)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most critical values across level of significance are well beyond the test statistic. This supports the Null hypothesis that the series is stationary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ADF and KPSS test conclusion "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second-order differencing data is found to be stationary by using ADF and KPSS test. Results from both tests are consistent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 White Noise Check "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test, we would test the autocorrelation between the current value its 12 lags. If there exist a correlation between the current value and a number of its lags, then the series is not white noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "def white_noise_test(input):\n",
    "    # Calculate the p-value of the autocorrelation\n",
    "    lags = 12\n",
    "    p_val_list = []\n",
    "    for i in range(1, lags):\n",
    "        result = acorr_ljungbox(input, lags= lags)\n",
    "        p_value = result.iloc[i-1,1]\n",
    "        p_val_list.append(p_value)\n",
    "    # check if all p_values in the list are below 0.05, then the time series is not a white noise\n",
    "    if all(i < 0.05 for i in p_val_list):\n",
    "        print('The time series is NOT a white noise.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_noise_test(second_order_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "def white_noise_test(input):\n",
    "    # Calculate the p-value of the autocorrelation\n",
    "    lags = 12\n",
    "    p_val_list = []\n",
    "    for i in range(1, lags + 1):\n",
    "        result = acorr_ljungbox(input, lags= lags)\n",
    "        p_value = result.iloc[i-1,1]\n",
    "        p_val_list.append(p_value.round(4))\n",
    "    # check if all p_values in the list are below 0.05, then the time series is not a white noise\n",
    "    if all(i < 0.05 for i in p_val_list):\n",
    "        print('The time series is NOT a white noise.')\n",
    "    else:\n",
    "        print('The time series is a white noise.')\n",
    "    # Store the p_values in a data frame\n",
    "    p_val_df = pd.DataFrame(p_val_list, index=range(1, lags+1), columns=['P_Value'])\n",
    "    return p_val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_noise_test(second_order_diff)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the series illustrate a correlation between the current value and its lags, the data is thus not white noise. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lag Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify the useful lag variables, we can use the autocorrelation function (ACF) and Partial Autocorrelation Function (PACF) plots."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference between ACF and PACF is that ACF measures the total correlation between a time series and its lagged values, while PACF measures the direct correlation between a time series and its lagged values after removing the effect of the correlations with the intervening observations. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACF is primarily used to determine the MA component, while the PACF plot is used to determine the AR component.\n",
    "\n",
    "The shaded area is the signifiance level in the ACF and PACF plots. If a lag is above the shaded area, it is significantly correlated with the label. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Label's ACF and PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF plot\n",
    "plot_acf(second_order_diff, lags= 24, zero=False)\n",
    "plt.title('ACF Plot of Second-Order Differenced Data')\n",
    "plt.show()\n",
    "\n",
    "# PACF plot\n",
    "plot_pacf(second_order_diff, lags = 24, zero=False)\n",
    "plt.title('PACF Plot of Second-Order Differenced Data')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Lag Analysis Conclusion "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The ACF plot shows that the label is correlated with its lagged values up to 3 periods.\n",
    "\n",
    "- Meanwhile, the PACF shows that the label is directly correlated with the first 4 lag values and lags of 9 and 22. We can't really be sure that lag 22 are really substantially significnnt as it shows on the graph due to the small size of the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Splitting the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training, validation, and test sets\n",
    "\n",
    "- 2010 - 2017 as training data\n",
    "\n",
    "- 2017 - 2019 as validation data\n",
    "\n",
    "- 2019 - 2020 as test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = second_order_diff.loc['2010-01-01':'2016-12-31']\n",
    "\n",
    "val = second_order_diff.loc['2017-01-01':'2018-12-31']\n",
    "\n",
    "test = second_order_diff.loc['2019-01-01':'2019-12-31']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation of train, val, and test sets and print the result out\n",
    "train_mean = train.mean().round(2)\n",
    "train_std = train.std().round(2)\n",
    "\n",
    "val_mean = val.mean().round(2)\n",
    "val_std = val.std().round(2)\n",
    "\n",
    "test_mean = test.mean().round(2)\n",
    "test_std = test.std().round(2)\n",
    "\n",
    "print('Train mean: ', train_mean)\n",
    "print('Train std: ', train_std)\n",
    "print('Val mean: ', val_mean)\n",
    "print('Val std: ', val_std)\n",
    "print('Test mean: ', test_mean)\n",
    "print('Test std: ', test_std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Modeling 1 (Lag Predictors only)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Base model: ARIMA(1,2,1)\n",
    "\n",
    "- The ARIMA(p,d,q) model contains 3 main components: AR, I (differencing), and MA.\n",
    "\n",
    "- After carefully taking into consideration, second-order differencing seems to be the best way to make the data stationary so decided to use it as the base model for comparision purpose.\n",
    "\n",
    "- The model takes into account 1 lagged values, 1 lagged errors, and 2 order differencing. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Model Executing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit an ARIMA(1,2,1) model to the training set\n",
    "\n",
    "#! Here we set I = 0 since we have manually differenced the data\n",
    "base_model = ARIMA(train, order=(1,0,1)).fit()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The lag of 1 component is found statistically insignificant since it has a very high p-value. Meanwhile, the AR component, which is the error term of the 1st lag. \n",
    "\n",
    "- The negative figure for skew and kurtosis also tell us about the distribution of the model's residuals as they are found to be skewed to the left and contain a fat tail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metrics on the train set\n",
    "train_pred = base_model.predict()\n",
    "train_rmse = np.sqrt(np.mean((train_pred - train)**2))\n",
    "train_mae = np.mean(np.abs(train_pred - train))\n",
    "\n",
    "print('Train RMSE: ', train_rmse)\n",
    "print('Train MAE: ', train_mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Predicting the Validation set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if possible, please repeat the mean, standard deviation of the label here (2nd-order differenced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast values for the validation set\n",
    "validation_forecast = base_model.forecast(steps=len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the forecasted values and the actual values and include evaluation metrics\n",
    "# Calculate evaluation metrics\n",
    "mae = np.mean(np.abs(validation_forecast - val))\n",
    "mse = np.mean((validation_forecast - val)**2)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(val, label='Actual')\n",
    "plt.plot(validation_forecast, label='Forecast')\n",
    "plt.text(0.88, 0.98, f'MAE: {mae:.2f}\\nMSE: {mse:.2f}\\nRMSE: {rmse:.2f}', \n",
    "                 transform=plt.gca().transAxes, verticalalignment='top')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Validation Set: Actual vs Forecast of ARIMA (1,2,1)')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Model Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "base_arima_test_pred = base_model.forecast(steps=36)\n",
    "\n",
    "# Only account for the last 12 months \n",
    "base_arima_test_pred = base_arima_test_pred[-12:]\n",
    "\n",
    "mae = np.mean(np.abs(base_arima_test_pred - test))\n",
    "mse = np.mean((base_arima_test_pred - test)**2)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"MAE: {mae:.2f}, MSE: {mse:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Calculate the variance of the base_arima_test_pred\n",
    "base_arima_test_pred_var = np.var(base_arima_test_pred)\n",
    "print('Variance of the base_arima_test_pred: ', base_arima_test_pred_var)\n",
    "\n",
    "# plot the forecasted values and the actual values and include evaluation metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(test, label='Actual')\n",
    "plt.plot(base_arima_test_pred, label='Forecast')\n",
    "plt.text(0.88, 0.98, f'MAE: {mae:.2f}\\nMSE: {mse:.2f}\\nRMSE: {rmse:.2f}',\n",
    "            transform=plt.gca().transAxes, verticalalignment='top')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Test Set: Actual vs Forecast of ARIMA (1,2,1)')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ARIMA with more ARs and MAs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From ACF and PACF results above, we were able to identify lags that are significantly correlated with the label, \n",
    "\n",
    "- ACF's result is helpful in determining MA components, while PACF's helps determine AR components"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs earlier, we would sequentially add MA and AR component to the model and observe how AIC and BIC change.\n",
    "\n",
    "- A lower BIC and AIC along with lower RMSE and MAE are preferred. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the lag analysis, we were able to figure out that the first 4 AR components and the first 3 residual lags appear to be stastistically significant to the model. Let's write a for loop to loop through the potential models and view the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = [2,3,4]\n",
    "ma = [1,2,3]\n",
    "\n",
    "for i in ma:\n",
    "\n",
    "    for j in ar:\n",
    "        \n",
    "        # train and fit the model\n",
    "        \n",
    "        model = ARIMA(train, order=(j,0,i)).fit(method_kwargs={'maxiter': 100})\n",
    "        \n",
    "        validation_forecast = model.forecast(steps=len(val))\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        \n",
    "        mae = np.mean(np.abs(validation_forecast - val))\n",
    "        \n",
    "        mse = np.mean((validation_forecast - val)**2)\n",
    "        \n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        # Plot the forecasted values and the actual values\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        plt.plot(val, label='Actual')\n",
    "        \n",
    "        plt.plot(validation_forecast, label='Forecast')\n",
    "        \n",
    "        plt.legend(loc='upper left')\n",
    "        \n",
    "        plt.title(f'Validation Set: Actual vs Forecast of ARIMA({j},2,{i})')\n",
    "        \n",
    "        plt.text(0.88, 0.98, f'MAE: {mae:.2f}\\nMSE: {mse:.2f}\\nRMSE: {rmse:.2f}', \n",
    "                 transform=plt.gca().transAxes, verticalalignment='top')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        # Attacht the model's summary right below the graph\n",
    "\n",
    "        print(model.summary())\n",
    "      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ARIMA(3,2,3) has the lowest RMSE, we will use it to forecast the test set. But first, let's extract the evaluation metrics on the train set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_arima = ARIMA(train, order=(3,0,3)).fit()\n",
    "\n",
    "# Prediction on the test set\n",
    "best_arima_test_pred = best_arima.forecast(steps=36)\n",
    "\n",
    "# Only account for the last 12 months\n",
    "\n",
    "best_arima_test_pred = best_arima_test_pred[-12:]\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "\n",
    "mae = np.mean(np.abs(best_arima_test_pred - test))\n",
    "\n",
    "mse = np.mean((best_arima_test_pred - test)**2)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Print evaluation metrics\n",
    "\n",
    "print(f\"MAE: {mae:.2f}, MSE: {mse:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Plot the forecasted values and the actual values\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(test, label='Actual')\n",
    "\n",
    "plt.plot(best_arima_test_pred, label='Forecast')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.text(0.88, 0.98, f'MAE: {mae:.2f}\\nMSE: {mse:.2f}\\nRMSE: {rmse:.2f}',\n",
    "            transform=plt.gca().transAxes, verticalalignment='top')\n",
    "\n",
    "plt.title('Test Set: Actual vs Forecast of ARIMA(3,2,3)')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the variance of the best_arima_test_pred\n",
    "\n",
    "best_arima_test_pred_var = np.var(best_arima_test_pred)\n",
    "\n",
    "print('Variance of the best_arima_test_pred: ', best_arima_test_pred_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metrics on the train set\n",
    "train_pred = best_arima.predict()\n",
    "train_rmse = np.sqrt(np.mean((train_pred - train)**2))\n",
    "train_mae = np.mean(np.abs(train_pred - train))\n",
    "\n",
    "print('Train RMSE: ', train_rmse)\n",
    "print('Train MAE: ', train_mae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ARIMA Model's Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Performance on the validation set*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The best ARIMA model so far is ARIMA(3,0,3). For some other ARIMA model versions, the maximum likelihood optimization method fails to converge. Therefore, it leads to poor predictions, as we can see there is a horizontal line for some ARIMA model's predictions, which is completely different than the ARIMA(3,0,3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *Performance on Test set*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Though model ARIMA(3,2,3) appears to have good predictive power on the validation dataset, it shows a poor performance on the test set as it underperforms the base model ARIMA(1,2,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Modeling II (Models With External Predictors) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing Predictors "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import and format data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import data with external predictors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = pd.read_csv('full_data.csv', index_col='Date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some basic infor from the data \n",
    "predictors.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the date consistent with the CPI data\n",
    "predictors = predictors.loc[:'2019-12-31']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Apply first order differencing on predictors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since we have taken differencing on CPI, it makes sense to take transform predictors to at least a first order differencing as well. Also, we would like to see how the change in these variables affect movement in the label.\n",
    "- Also, as I have attempted to use the original data, the multicollinarity issue was so serious that we can't move forward with it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply diff on all columns in predictors \n",
    "predictors = predictors.diff().dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Normalize Predictors "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Remove Outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All predictors are deemed to be equally important but they appear to be on different scale, thus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data by plotting their distributions and boxplots\n",
    "# sns.pairplot(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all outliers in the predictors file using IQR method\n",
    "def replace_outliers(data):\n",
    "    for col in data.columns:\n",
    "        q1 = data[col].quantile(0.25)\n",
    "        q3 = data[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        \n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        \n",
    "        data[col] = np.where(data[col] < lower_bound, lower_bound, data[col])\n",
    "        data[col] = np.where(data[col] > upper_bound, upper_bound, data[col])\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_predictors = replace_outliers(predictors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Normalize predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize clean predictors data using min-max scaler, and convert it to a dataframe\n",
    "# import scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "clean_predictors = scaler.fit_transform(clean_predictors)\n",
    "clean_predictors = pd.DataFrame(clean_predictors, columns=predictors.columns, index=predictors.index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been cleaned, we can merge them with the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the clean predictors data with the CPI data\n",
    "full_data = pd.merge(second_order_diff, clean_predictors, left_index=True, right_index=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = full_data.corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "plt.figure(figsize=(13, 9))\n",
    "sns.heatmap(corr_matrix, mask=mask, cmap='coolwarm_r', annot=True, fmt='.2f', annot_kws={'fontsize': 8.5})\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most features are moderately or weakly correlated with CPI. In economic sense, they should have a strong correlation with the label, however, since we have differenced both label and features, the strong correlation no longer holds. \n",
    "\n",
    "- Though some features like Money_Stock (M2 money supply) and FedSurDef are found to have a small correlation with the label, it might still be useful based on our domain knowledge. \n",
    " \n",
    "- In addition, since correlation measures only linear relationships, non-linear relationships between predictors and lable can still be significant and useful for prediction and they won't be captured by correlation coefficients. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection with Lasso Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Though the current set of variables look good. Next, we apply Lasso Regression to filter the number of predictors even further in order to retain the most important variables only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LassoCV\n",
    "# split the data \n",
    "target = 'CPI'\n",
    "\n",
    "train = full_data.loc['2010-01-01':'2016-12-31']\n",
    "\n",
    "val = full_data.loc['2017-01-01':'2018-12-31']\n",
    "\n",
    "test = full_data.loc['2019-01-01':'2019-12-31']\n",
    "\n",
    "x_train = train.drop(columns = [target])\n",
    "\n",
    "y_train = train[target]\n",
    "\n",
    "x_val = val.drop(columns = [target])\n",
    "\n",
    "y_val = val[target]\n",
    "\n",
    "x_test = test.drop(columns = [target])\n",
    "\n",
    "y_test = test[target]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best alpha as performed below is the one that provides the optimal balance between fitting the data and preventing overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit a lasso regression with cross validation to find the best alpha\n",
    "model = LassoCV(alphas = None, cv = 3, random_state=123).fit(x_train, y_train)\n",
    "\n",
    "best_alpha = model.alpha_\n",
    "\n",
    "print(f\"Best alpha: {best_alpha:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Though we have found the best alpha, we are unable to apply it to the lasso regresion since it would only keep Crude oil as the sole predictor for the model. \n",
    "\n",
    "- Therfore, we reduce alpha to 0.01, while maintaining the same RMSE but it include more predictors for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can fit the model with the best alpha\n",
    "final_lasso = Lasso(alpha=0.01, random_state=123).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model performance on the validation set \n",
    "val_predictions = final_lasso.predict(x_val)\n",
    "val_mse = mean_squared_error(y_val, val_predictions)\n",
    "val_rmse = np.sqrt(val_mse)\n",
    "print(f'Validation RMSE: {val_rmse:.2f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insepct the coefficients to see which predictors were retained in the model \n",
    "coef_df = pd.DataFrame({'Feature': x_train.columns, 'Coefficient': final_lasso.coef_})\n",
    "coef_df = coef_df.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "# print Feature from coef_df where Coefficient is different from 0\n",
    "\n",
    "print('Here is the list of predictors that were retained in the lasso regression using alpha = 0.01')\n",
    "\n",
    "coef_df[coef_df['Coefficient'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a vector names for these retained variables. \n",
    "selected_predictors = coef_df[coef_df['Coefficient'] != 0]['Feature'].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for random forest regression \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Random Forest with No Lags"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Default Setting's Hyperparameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a base random forest regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_base = RandomForestRegressor(random_state=123)\n",
    "\n",
    "# Train the base model \n",
    "\n",
    "rf_base.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print evaluation metrics on training set\n",
    "train_predictions = rf_base.predict(x_train)\n",
    "train_mse = mean_squared_error(y_train, train_predictions)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = np.mean(np.abs(train_predictions - y_train))\n",
    "print(f'Train RMSE: {train_rmse:.2f}\\n')\n",
    "print(f'Train MAE: {train_mae:.2f}\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and Evaluate the model's metrics on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the validation target variable\n",
    "base_rf_pred = rf_base.predict(x_val)\n",
    "\n",
    "# Evaluation \n",
    "\n",
    "mse = mean_squared_error(y_val, base_rf_pred)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "mae = np.mean(np.abs(base_rf_pred - y_val))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Actual and Predicted Values of random forest model with default setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_default_setting_predictions(actual, predicted, title):\n",
    "    \"\"\"\n",
    "    Plots the actual and predicted values of a time series.\n",
    "    \n",
    "    Args:\n",
    "        actual (series): The actual values of the time series\n",
    "        predicted (series): The predicted values of the time series\n",
    "        title (string): The title of the plot\n",
    "    \"\"\"\n",
    "    # Adding the index of the actual series to the predicted series \n",
    "    predicted = pd.Series(predicted, index=actual.index)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(actual, label='Actual')\n",
    "    plt.plot(predicted, label='Predicted')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.text(0.88, 0.98, f'MAE: {mae:.2f}\\nMSE: {mse:.2f}\\nRMSE: {rmse:.2f}', \n",
    "                 transform=plt.gca().transAxes, verticalalignment='top')\n",
    "    plt.title(title)\n",
    "    # Create a small subtitle with a different color font\n",
    "    plt.text(0.34, 1.1, \"Default Setting's Hyperparameters\", color='red', transform=plt.gca().transAxes, verticalalignment='top')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_default_setting_predictions(y_val, base_rf_pred, 'Random Forest Regression Without Lags')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions on testing sets and evaluate the model's metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test target variable\n",
    "base_rf_test = rf_base.predict(x_test)\n",
    "\n",
    "# Evaluation \n",
    "\n",
    "mse = mean_squared_error(y_test, base_rf_test)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "mae = np.mean(np.abs(base_rf_test - y_test))\n",
    "\n",
    "# Calculate the variance of the test prediction\n",
    "var = np.var(base_rf_test)\n",
    "\n",
    "print(f\"Test Prediction's Variance: {var:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_default_setting_predictions(y_test, base_rf_test, \"Test Set: Actual vs Forecast of Random Forest Regression Without Label's Lags\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse the prediction back to the original data to compare with the monthly CPI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to reverse the second order differenced data back to the original data. \n",
    "def reconstruct_second_order_differenced_data(input):\n",
    "    reconstructed_data = [df['CPI'].loc['2019-01-01'], df['CPI'].loc['2019-02-01']]\n",
    "    actual_data = df['CPI'].loc['2019-01-01':'2019-12-01']\n",
    "    for i in range(10):\n",
    "        original_value = input[i+1] + 2 * actual_data[i+1] - actual_data[i]\n",
    "        reconstructed_data.append(original_value)\n",
    "    return reconstructed_data[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the result of the previous function with df['1-Month % Change']\n",
    "def merge_with_1_month_pct_change(second_order_diff_data):\n",
    "    reconstructed_data = reconstruct_second_order_differenced_data(second_order_diff_data)\n",
    "    actual_data = df['CPI'].loc['2019-02-01':'2019-12-01']\n",
    "    # calculate the percentage difference in the reconstructed_data\n",
    "    reconstructed_data = [((reconstructed_data[i] - actual_data[i]) / actual_data[i]) * 100 for i in range(0, len(reconstructed_data))]\n",
    "    reconstructed_data = pd.Series(reconstructed_data, index=df['1-Month % Change'].loc['2019-03-01':].index)\n",
    "    reconstructed_data.name = 'Predicted'\n",
    "    reconstructed_data = reconstructed_data.round(1)\n",
    "    reconstructed_data = pd.merge(reconstructed_data, df['1-Month % Change'].loc['2019-03-01':], left_index=True, right_index=True)\n",
    "    reconstructed_data.columns = ['Predicted', 'Actual']\n",
    "    return reconstructed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_with_1_month_pct_change(base_rf_test)\n",
    "\n",
    "# Calculat the variance of "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Tuned Hyperparameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are two common fine-tunning method for random forest: Grid Search and Random Search."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameter search space for both grid search random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search space\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [10, 20, 30, 50, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'bootstrap': [True, False],\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chooose a search method (GridSearchCV or RandomizedSearchCV) and fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_base, \n",
    "    param_grid=param_grid, \n",
    "    cv=3, \n",
    "    n_jobs=-1, \n",
    "    verbose=2)\n",
    "\n",
    "# Random search\n",
    "\n",
    "# n_iter: Number of random parameter combinations to try\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=rf_base, \n",
    "    param_distributions=param_grid, \n",
    "    n_iter=100, \n",
    "    cv=5, \n",
    "    n_jobs=-1, \n",
    "    verbose=2, \n",
    "    random_state=123)\n",
    "\n",
    "# Fit the search object, here we can use either random search or grid searchq\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "random_search.fit(x_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the best hyperparameters from the both hyperparameter search methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_params = grid_search.best_params_\n",
    "\n",
    "random_search_params = random_search.best_params_\n",
    "\n",
    "\n",
    "# # Create a data frame that combines both grid_search_params and random_search_params\n",
    "grid_search_params_df = pd.DataFrame(grid_search_params, index=[0])\n",
    "\n",
    "random_search_params_df = pd.DataFrame(random_search_params, index=[0])\n",
    "\n",
    "combined_params_df = pd.concat([grid_search_params_df, random_search_params_df], axis=0)\n",
    "\n",
    "combined_params_df.index = ['Grid Search', 'Random Search']\n",
    "\n",
    "combined_params_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with the best hyperparameters\n",
    "tunned_rf_regressor = RandomForestRegressor(**grid_search_params, random_state=123)\n",
    "\n",
    "# Train the model \n",
    "tunned_rf_regressor.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluation metrics on the training set\n",
    "train_predictions = tunned_rf_regressor.predict(x_train)\n",
    "train_mse = mean_squared_error(y_train, train_predictions)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = np.mean(np.abs(train_predictions - y_train))\n",
    "print(f'Train RMSE: {train_rmse:.2f}\\n')\n",
    "print(f'Train MAE: {train_mae:.2f}\\n')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predicitons and evaluate the model performance using RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = tunned_rf_regressor.predict(x_val)\n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "mae = np.mean(np.abs(y_pred - y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot y_val and y_pred on the same graph, but first, we need to add a time index to y_pred\n",
    "y_pred = pd.Series(y_pred, index=y_val.index)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(y_val, label='Actual')\n",
    "\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.text(0.88, 0.98, f'MAE: {mae:.2f}\\nMSE: {mse:.2f}\\nRMSE: {rmse:.2f}', \n",
    "                 transform=plt.gca().transAxes, verticalalignment='top')\n",
    "\n",
    "plt.text(0.35, 1.1, 'Tunned Hyperparameters', \n",
    "         color='red', \n",
    "         transform=plt.gca().transAxes, \n",
    "         verticalalignment='top')\n",
    "\n",
    "plt.title('Random Forest Regression Without Lags')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions and Evaluation metrics on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = tunned_rf_regressor.predict(x_test)\n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "mae = np.mean(np.abs(y_pred - y_test))\n",
    "\n",
    "# Calculate the variance on test prediction \n",
    "var = np.var(y_pred)\n",
    "\n",
    "print(f\"Varaince of Test Prediction on Tuned RF w/o Lags: {var:.2f}\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the prediction and acutal values of the testing prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot y_val and y_pred on the same graph, but first, we need to add a time index to y_pred\n",
    "def plot_tunned_predictions_test(actual, predicted, title):\n",
    "    predicted = pd.Series(predicted, index=actual.index)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    plt.plot(actual, label='Actual')\n",
    "\n",
    "    plt.plot(predicted, label='Predicted')\n",
    "\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.text(0.88, 0.98, f'MAE: {mae:.2f}\\nMSE: {mse:.2f}\\nRMSE: {rmse:.2f}', \n",
    "                    transform=plt.gca().transAxes, verticalalignment='top')\n",
    "\n",
    "    plt.text(0.35, 1.1, 'Tunned Hyperparameters', \n",
    "            color='red', \n",
    "            transform=plt.gca().transAxes, \n",
    "            verticalalignment='top')\n",
    "\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tunned_predictions_test(y_test, y_pred, \"Test Set: Actual vs Forecast Random Forest Regression Without Label's Lags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_with_1_month_pct_change(y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Random Forest with No Lags Conclusion\n",
    "\n",
    "- Since the search space and the data are quite small, we can move forward with Grid Search CVsdsad\n",
    "\n",
    "1. Validation set\n",
    "\n",
    "- Though MAE in tunned hyperparameter's model is 1 basis point lower than the default setting's model, the RMSE and MSE remain the same.\n",
    "\n",
    "2. Test set\n",
    "- The model's performance on test set of the tunned hyperparameters are slightly worse than the default setting's model. \n",
    "\n",
    "As a result, we can conclude that tunning hyperparameters does not improve the model's performance. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Random Forest with Label's Lags"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the best ARIMA model, which is ARIMA(3,0,3), we can see that the first 3 lags appear to be statistically significant to predict the CPI, therefore, we decide include them to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function create lag features for a time series\n",
    "def create_lag_features(df, target, lags):\n",
    "    \"\"\"\n",
    "    Creates lag features for a time series.\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): A dataframe containing the time series data\n",
    "        target (string): The column name of the target variable\n",
    "        lags (list): A list of lag values to create features for\n",
    "        \n",
    "    Returns:\n",
    "        The original dataframe with added columns containing lag features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    for lag in lags:\n",
    "        df['lag_' + str(lag)] = df[target].shift(lag)\n",
    "           \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 3 lags to the full_data dataset\n",
    "full_data_w_lags = create_lag_features(full_data, target, [1, 2, 3])\n",
    "\n",
    "full_data_w_lags.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training, validation, and test sets again \n",
    "train_w_lags = full_data_w_lags.loc['2010-01-01':'2016-12-31']\n",
    "\n",
    "val_w_lags = full_data_w_lags.loc['2017-01-01':'2018-12-31']\n",
    "\n",
    "test_w_lags = full_data_w_lags.loc['2019-01-01':'2019-12-31']\n",
    "\n",
    "x_train_w_lags = train_w_lags.drop(columns = [target])\n",
    "\n",
    "y_train_w_lags = train_w_lags[target]\n",
    "\n",
    "x_val_w_lags = val_w_lags.drop(columns = [target])\n",
    "\n",
    "y_val_w_lags = val_w_lags[target]\n",
    "\n",
    "x_test_w_lags = test_w_lags.drop(columns = [target])\n",
    "\n",
    "y_test_w_lags = test_w_lags[target]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the base Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_w_lags = RandomForestRegressor(random_state=123)\n",
    "\n",
    "# Train the base model \n",
    "rf_w_lags.fit(x_train_w_lags, y_train_w_lags)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Default's Setting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the training set\n",
    "train_predictions = rf_w_lags.predict(x_train_w_lags)\n",
    "train_mse = mean_squared_error(y_train_w_lags, train_predictions)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = np.mean(np.abs(train_predictions - y_train_w_lags))\n",
    "print(f'Train RMSE: {train_rmse:.2f}\\n')\n",
    "print(f'Train MAE: {train_mae:.2f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = rf_w_lags.predict(x_val_w_lags)\n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "mse = mean_squared_error(y_val_w_lags, y_pred)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "mae = np.mean(np.abs(y_pred - y_val_w_lags))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the actual values and prediction of the model with default setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_default_setting_predictions(y_val_w_lags, y_pred, 'Random Forest Regression With Lags')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction and Evaluation metrics on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction and Evaluation metrics on test set\n",
    "y_pred = rf_w_lags.predict(x_test_w_lags)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "mae = np.mean(np.abs(y_pred - y_test))\n",
    "\n",
    "# Calculate the variance on test prediction\n",
    "\n",
    "var = np.var(y_pred)\n",
    "\n",
    "print(f\"Variance of Test Prediction on RF w/ Lags: {var:.2f}\\n\")\n",
    "\n",
    "plot_default_setting_predictions(y_test, y_pred, \"Test Set: Actual vs Forecast of Random Forest Regression With Label's Lags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_with_1_month_pct_change(y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Tuned Hyperparameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameter search space for grid search or random search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_w_lags, \n",
    "    param_grid=param_grid, \n",
    "    cv=3, \n",
    "    n_jobs=-1, \n",
    "    verbose=2)\n",
    "\n",
    "# Random search\n",
    "\n",
    "# n_iter: Number of random parameter combinations to try\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=rf_w_lags, \n",
    "    param_distributions=param_grid, \n",
    "    n_iter=100, \n",
    "    cv=5, \n",
    "    n_jobs=-1, \n",
    "    verbose=2, \n",
    "    random_state=123)\n",
    "\n",
    "# Fit the search object, here we can use either random search or grid searchq\n",
    "\n",
    "grid_search.fit(x_train_w_lags, y_train_w_lags)\n",
    "\n",
    "random_search.fit(x_train_w_lags, y_train_w_lags)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the optimal hyperparameters both grid search and random search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_params = grid_search.best_params_\n",
    "\n",
    "random_search_params = random_search.best_params_\n",
    "\n",
    "# # Create a data frame that combines both grid_search_params and random_search_params\n",
    "grid_search_params_df = pd.DataFrame(grid_search_params, index=[0])\n",
    "\n",
    "random_search_params_df = pd.DataFrame(random_search_params, index=[0])\n",
    "\n",
    "combined_params_df = pd.concat([grid_search_params_df, random_search_params_df], axis=0)\n",
    "\n",
    "combined_params_df.index = ['Grid Search', 'Random Search']\n",
    "\n",
    "combined_params_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with grid search hyperparameters\n",
    "grid_search_rf_regressor = RandomForestRegressor(**grid_search_params, random_state=123)\n",
    "\n",
    "# Train the model \n",
    "grid_search_rf_regressor.fit(x_train_w_lags, y_train_w_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics on training set\n",
    "train_predictions = grid_search_rf_regressor.predict(x_train_w_lags)\n",
    "train_mse = mean_squared_error(y_train_w_lags, train_predictions)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = np.mean(np.abs(train_predictions - y_train_w_lags))\n",
    "print(f'Train RMSE: {train_rmse:.2f}\\n')\n",
    "print(f'Train MAE: {train_mae:.2f}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction and Evaluation on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "grid_search_y_pred = grid_search_rf_regressor.predict(x_val_w_lags)\n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "mse = mean_squared_error(y_val_w_lags, grid_search_y_pred)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "mae = np.mean(np.abs(grid_search_y_pred - y_val_w_lags))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Actual and Predicted Values of Grid Search Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to visualize actual and predicted values\n",
    "def plot_grid_search_predictions(actual, predicted, title):\n",
    "    \"\"\"\n",
    "    Plots the actual and predicted values of a time series.\n",
    "    \n",
    "    Args:\n",
    "        actual (series): The actual values of the time series\n",
    "        predicted (series): The predicted values of the time series\n",
    "        title (string): The title of the plot\n",
    "    \"\"\"\n",
    "    # Adding the index of the actual series to the predicted series \n",
    "    predicted = pd.Series(predicted, index=actual.index)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(actual, label='Actual')\n",
    "    plt.plot(predicted, label='Predicted')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.text(0.88, 0.98, f'MAE: {mae:.2f}\\nMSE: {mse:.2f}\\nRMSE: {rmse:.2f}', \n",
    "                 transform=plt.gca().transAxes, verticalalignment='top')\n",
    "    plt.title(title)\n",
    "    # Create a small subtitle with a different color font\n",
    "    plt.text(0.41, 1.1, 'Grid Search', color='red', transform=plt.gca().transAxes, verticalalignment='top')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_search_predictions(y_val_w_lags, grid_search_y_pred, 'Optimized Random Forest Regression Including Lags')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction and Evaluation metrics on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction and Evaluation metrics on test set\n",
    "y_pred = grid_search_rf_regressor.predict(x_test_w_lags)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "mae = np.mean(np.abs(y_pred - y_test))\n",
    "\n",
    "# Calculate the variance on test prediction\n",
    "\n",
    "var = np.var(y_pred)\n",
    "\n",
    "print(f\"Variance of Test Prediction on tunned RF w/ Lags: {var:.2f}\\n\")\n",
    "\n",
    "plot_tunned_predictions_test(y_test, y_pred, \"Test Set: Actual vs Forecast of Random Forest Regression With Label's Lags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_with_1_month_pct_change(y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with random search hyperparameters \n",
    "random_search_rf_regressor = RandomForestRegressor(**random_search_params, random_state = 123)\n",
    "\n",
    "# Fit the model \n",
    "\n",
    "random_search_rf_regressor.fit(x_train_w_lags, y_train_w_lags)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction and Evaluation on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions \n",
    "random_search_y_pred = random_search_rf_regressor.predict(x_val_w_lags)\n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "mse = mean_squared_error(y_val_w_lags, random_search_y_pred)\n",
    "\n",
    "rmse  = np.sqrt(mse)\n",
    "\n",
    "mae = np.mean(np.abs(random_search_y_pred - y_val_w_lags))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Actual and Predicted values of Random Search Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to visualize actual and predicted values\n",
    "def plot_random_search_predictions(actual, predicted, title):\n",
    "    \"\"\"\n",
    "    Plots the actual and predicted values of a time series.\n",
    "    \n",
    "    Args:\n",
    "        actual (series): The actual values of the time series\n",
    "        predicted (series): The predicted values of the time series\n",
    "        title (string): The title of the plot\n",
    "    \"\"\"\n",
    "    # Adding the index of the actual series to the predicted series \n",
    "    predicted = pd.Series(predicted, index=actual.index)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(actual, label='Actual')\n",
    "    plt.plot(predicted, label='Predicted')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.text(0.88, 0.98, f'MAE: {mae:.2f}\\nMSE: {mse:.2f}\\nRMSE: {rmse:.2f}', \n",
    "                 transform=plt.gca().transAxes, verticalalignment='top')\n",
    "    plt.title(title)\n",
    "    # Create a small subtitle with a different color font\n",
    "    plt.text(0.41, 1.1, 'Random Search', color='red', transform=plt.gca().transAxes, verticalalignment='top')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_search_predictions(y_val_w_lags, random_search_y_pred, 'Optimized Random Forest Regression Including Lags')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions and Evaluation metrics on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions \n",
    "random_search_y_pred_test = random_search_rf_regressor.predict(x_test_w_lags)\n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "mse = mean_squared_error(y_test, random_search_y_pred_test)\n",
    "\n",
    "rmse  = np.sqrt(mse)\n",
    "\n",
    "mae = np.mean(np.abs(random_search_y_pred_test - y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_random_search_predictions(y_test, random_search_y_pred_test, \"Test Set: Actual vs Forecast of Random Forest Regression Including Label's Lags\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back transform the predictions to the original data and compare with the actual monthly rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_with_1_month_pct_change(random_search_y_pred_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Random Forest Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regarding the model's performance, the model with lags performs better than the model without lags.\n",
    "\n",
    "- The model with lags also has a lower RMSE than the model without lags.\n",
    "\n",
    "- The model with lags also has a lower RMSE than the best ARIMA model, which is ARIMA(3,0,3).\n",
    "\n",
    "- Regarding model without lags, tunning the hyperparameters DOES NOT improve the model's performance. It is indicated by the fact that the RMSE of the model with default setting and the model with tunning hyperparameters are identical, though MAE of the tunned Forest is slightly lower than the default Forest (0.39 vs 0.40).\n",
    "\n",
    "- Regarding model with lags (lag1, lag2, and lag3), tunning the hyperparameters DOES improve the model's performance. It is indicated by the fact that the RMSE of tunning hyperparameters of both grid search and random search are both moderately lower than in default settting model. \n",
    "\n",
    "- The model yield the greatest performance is the model with lags and hyperparameters from grid search, with the RMSE of 0.42. \n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Long Short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Reformatting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with LSTM, we need to reshape the data into 3D array. The 3D input array for an LSTM has the following dimensions: \n",
    "\n",
    "- Samples: The total number of observations in the dataset.\n",
    "\n",
    "- Time Steps: The total number of time steps in the input data.\n",
    "\n",
    "- Features: The total number of features in the input data that we would like to include in the model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Long Short Term Memory model, we set time steps equal 1 because we have already include the 3 lags in the training data. In addition, we would like to focus on learning patterns between features rather than across time between these features and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the input data into 3D format for LSTM: (samples, timesteps, features)\n",
    "lstm_x_train = x_train_w_lags.values.reshape((x_train_w_lags.shape[0], 1, x_train_w_lags.shape[1]))\n",
    "lstm_x_val = x_val_w_lags.values.reshape((x_val_w_lags.shape[0], 1, x_val_w_lags.shape[1]))\n",
    "lstm_x_test = x_test_w_lags.values.reshape((x_test_w_lags.shape[0], 1, x_test_w_lags.shape[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Tune Hyperparameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a few samples for batch sizes, drop out rates, and hidden nodes and loop over all of them to find the best combination of hyperparameters.\n",
    "\n",
    "Though we set epoch equal to 100, the model would stop training if the validation loss does not improve after 3 epochs since we have turned on the early stopping callback and a patience of 3.\n",
    "\n",
    "To be specific, the training will stop if the validation loss does not improve for 3 consecutive epochs.The best weights during training will be restored to the model to ensure that we obtain the best performance for the given hyperparameters. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One epoch is completed when all batches in the dataset have been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "seed_value = 123\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "random.seed(seed_value)\n",
    "\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "batch_sizes = [8, 12, 16, 32]\n",
    "dropout_rates = [0.1, 0.15, 0.2, 0.3]\n",
    "hidden_nodes = [10, 20, 30, 50]\n",
    "\n",
    "best_params = {\n",
    "    \"batch_size\": batch_sizes[0],\n",
    "    \"dropout_rate\": dropout_rates[0],\n",
    "    \"hidden_nodes\": hidden_nodes[0]\n",
    "}\n",
    "lowest_val_loss = float(\"inf\")\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for dropout_rate in dropout_rates:\n",
    "        for hidden_node in hidden_nodes:\n",
    "            print(f\"Training with batch_size = {batch_size}, dropout_rate = {dropout_rate}, hidden_nodes = {hidden_node}\")\n",
    "\n",
    "            lstm_2 = Sequential()\n",
    "            lstm_2.add(LSTM(units=hidden_node, activation='tanh', input_shape=(lstm_x_train.shape[1], lstm_x_train.shape[2])))\n",
    "            lstm_2.add(Dropout(dropout_rate))\n",
    "            lstm_2.add(Dense(1))\n",
    "\n",
    "            optimizer = Adam(learning_rate=0.001)\n",
    "            lstm_2.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "            history = lstm_2.fit(lstm_x_train, \n",
    "                                 y_train_w_lags, \n",
    "                                 epochs=100, \n",
    "                                 batch_size=batch_size, \n",
    "                                 validation_data=(lstm_x_val, y_val_w_lags), \n",
    "                                 verbose=0, \n",
    "                                 shuffle=False, \n",
    "                                 callbacks=[early_stopping])\n",
    "\n",
    "            current_val_loss = min(history.history['val_loss'])\n",
    "            print(f\"Lowest validation loss: {current_val_loss}\\n\")\n",
    "\n",
    "            if current_val_loss < lowest_val_loss:\n",
    "                lowest_val_loss = current_val_loss\n",
    "                best_params = {\n",
    "                    \"batch_size\": batch_size,\n",
    "                    \"dropout_rate\": dropout_rate,\n",
    "                    \"hidden_nodes\": hidden_node\n",
    "                }\n",
    "\n",
    "print(f\"Best hyperparameters: {best_params}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the loop, we have found that the batch_size of 32, dropout rate of 0.2, and 10 hidden nodes yield the best performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "seed_value = 123\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "random.seed(seed_value)\n",
    "\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "lstm_2 = Sequential()\n",
    "\n",
    "lstm_2.add(LSTM(units=10, activation='tanh', input_shape=(lstm_x_train.shape[1], lstm_x_train.shape[2])))\n",
    "\n",
    "lstm_2.add(Dropout(0.1))\n",
    "\n",
    "lstm_2.add(Dense(1))\n",
    "\n",
    "lstm_2.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# store the model history in a variable to plot the learning curve vs epochs\n",
    "\n",
    "history = lstm_2.fit(lstm_x_train, \n",
    "                    y_train_w_lags, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    validation_data=(lstm_x_val, y_val_w_lags), \n",
    "                    verbose=0, \n",
    "                    shuffle=False, \n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Loss vs. Epochs of training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics on the training set\n",
    "\n",
    "train_predictions = lstm_2.predict(lstm_x_train)\n",
    "\n",
    "# convert train_predictions to a 1D array\n",
    "train_predictions = train_predictions.flatten()\n",
    "\n",
    "train_mse = mean_squared_error(y_train_w_lags, train_predictions)\n",
    "\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "\n",
    "train_mae = np.mean(np.abs(train_predictions - y_train_w_lags))\n",
    "\n",
    "print(f'Train RMSE: {train_rmse:.2f}\\n')\n",
    "\n",
    "print(f'Train MAE: {train_mae:.2f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train_predictions and y_train_w_lags on the same graph, but first add the time index to train_predictions\n",
    "\n",
    "train_predictions = pd.Series(train_predictions, index=y_train_w_lags.index)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(y_train_w_lags, label='Actual')\n",
    "\n",
    "plt.plot(train_predictions, label='Predicted')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.title('LSTM Regression Without Lags')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Validation Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_val_pred = lstm_2.predict(lstm_x_val)\n",
    "\n",
    "# Convert lstm_val_pred to a 1D array\n",
    "\n",
    "lstm_val_pred = lstm_val_pred.flatten()\n",
    "\n",
    "mse = mean_squared_error(y_val_w_lags, lstm_val_pred)  \n",
    "\n",
    "mae = np.mean(np.abs(lstm_val_pred - y_val_w_lags.values))\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_val_w_lags, lstm_val_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Prediction vs. Actual Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_plot(actual, predicted, title):\n",
    "    \"\"\"\n",
    "    Plots the actual and predicted values of this lstm result.\n",
    "    \n",
    "    Args:\n",
    "        actual (series): The actual values of the time series\n",
    "        predicted (series): The predicted values of the time series\n",
    "        title (string): The title of the plot\n",
    "    \"\"\"\n",
    "    # Adding the index of the actual series to the predicted series \n",
    "    predicted = pd.Series(predicted.reshape(-1), index=actual.index)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(actual, label='Actual')\n",
    "    plt.plot(predicted, label='Predicted')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.text(0.88, 0.98, f'MAE: {mae:.2f}\\nMSE: {mse:.2f}\\nRMSE: {rmse:.2f}', \n",
    "                 transform=plt.gca().transAxes, verticalalignment='top')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_plot(y_val_w_lags, lstm_val_pred, 'LSTM Model Including Lags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the prediction on test set\n",
    "lstm_x_test = x_test_w_lags.values.reshape((x_test_w_lags.shape[0], 1, x_test_w_lags.shape[1]))\n",
    "\n",
    "lstm_test_pred = lstm_2.predict(lstm_x_test)\n",
    "\n",
    "# convert lstm_test_pred to a 1D array\n",
    "\n",
    "lstm_test_pred = lstm_test_pred.flatten()\n",
    "\n",
    "mse = mean_squared_error(y_test, lstm_test_pred)\n",
    "\n",
    "mae = np.mean(np.abs(lstm_test_pred - y_test.values))\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, lstm_test_pred))\n",
    "\n",
    "# Calculate the variance on test prediction\n",
    "\n",
    "var = np.var(lstm_test_pred)\n",
    "\n",
    "print(f\"Variance of Test Prediction on LSTM w/ Lags: {var:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize teh prediction on test set\n",
    "lstm_plot(y_test, lstm_test_pred, \"Test Set: Actual vs Forecast of LSTM Model With Label's Lags\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 LSTM Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After the tunning process, we were able to achieve the best RMSE among all machine learning method so far. However, its MAE actually underperforms all random forest models and ARIMA(3,0,3)\n",
    "\n",
    "- LSTM does seem to catch the movement of the actual data well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_with_1_month_pct_change(lstm_test_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Garch Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Generalized Autogregressive Conditional Heteroskedasticity (GARCH) model is a populuar tool for estimating and forecasting volatility in time series data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Garch models only help predict the volatility, we would combine it with the best ARIMA model to see predicted values and the prediction interval. \n",
    "\n",
    "A prediction interval is a range of values that is likely to contain the actual future value of the time series. The wider the interval, the higher the uncertainty or risk. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In GARCH model, p and q are parameters that define the order of the model. They control how many lagged values of the squared residuals (for p) and the conditional variance (for q) are used in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from arch import arch_model\n",
    "\n",
    "# # fit ARIMA(3,0,3)\n",
    "\n",
    "# arima_3_0_3 = ARIMA(train['CPI'], order=(3,0,3)).fit()\n",
    "\n",
    "# # extract residual from the ARIMA(3,0,3) model\n",
    "\n",
    "# arima_3_0_3_residuals = arima_3_0_3.resid\n",
    "\n",
    "# # fit GARCH(1,1) model on the residuals of the ARIMA model \n",
    "# garch = arch_model(arima_3_0_3_residuals, p=1, q=1).fit()\n",
    "\n",
    "\n",
    "# # use ARIMA to predict mean \n",
    "\n",
    "# predicted_mean = arima_3_0_3.predict(n_periods= 2)[0]\n",
    "\n",
    "# # use Garch to predict the residual \n",
    "\n",
    "# garch_forecast = garch.forecast(horizon=2)\n",
    "\n",
    "# predict_et = garch_forecast.mean['h.1'].iloc[-1]\n",
    "\n",
    "# # combine the predicted mean and predicted residual to get the final prediction\n",
    "\n",
    "# arima_garch_pred = predicted_mean + predict_et\n",
    "\n",
    "# #plot the prediction and actual value on the same graph\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plt.plot(y_val, label='Actual')\n",
    "\n",
    "# plt.plot(arima_garch_pred, label='Predicted')\n",
    "\n",
    "# plt.legend(loc='upper left')\n",
    "\n",
    "# plt.title('ARIMA-GARCH Model')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Rolling Forecast Origin in ARIMA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea behind this method is to split your dataset into a training set, validation, and test set. However,instead of a simple static split, you use a series of \"windows\" that roll through the data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have a time series data from 2010 to 2016. You could decide to use the data from 2010 to 2016 to train your model and then test the model on data from 2019-2020 (2017-2019 for validation). But in a rolling forecast origin approach, you might start by training your model on data from 2010 to 2016, and then use this model to forecast the value for the first point in 2017. Then you would expand your training data to include the first point in 2017, refit the model, and forecast the second point in 2017, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rolling forecast origin for ARIMA(1,2,1) model\n",
    "\n",
    "# Create the model\n",
    "\n",
    "predictions_rolling = pd.Series()\n",
    "\n",
    "for end_date in test.index:\n",
    "    rolled_data = second_order_diff.loc[:end_date]\n",
    "    model = ARIMA(rolled_data, order=(1,0,1)).fit()\n",
    "    pred = model.forecast(horizon=1)\n",
    "    predictions_rolling.loc[end_date] = pred.values[0]\n",
    "\n",
    "# after finish the loop, add the index to predictions_rolling\n",
    "predictions_rolling.index = test.index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize residual rolling and prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_rolling = second_order_diff - predictions_rolling\n",
    "\n",
    "# plot the residuals\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(residual_rolling)\n",
    "\n",
    "plt.title('Residuals of Rolling Forecast Origin')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot prediction against actual values of the validation set, include MAE, MSE, RMSE\n",
    "\n",
    "mae = np.mean(np.abs(predictions_rolling - test['CPI']))\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(test['CPI'], predictions_rolling))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(test['CPI'], label='Actual')\n",
    "\n",
    "plt.plot(predictions_rolling, label='Predicted')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.text(0.88, 0.98, f'MAE: {mae:.2f}\\nMSE: {mse:.2f}\\nRMSE: {rmse:.2f}',\n",
    "            transform=plt.gca().transAxes, verticalalignment='top')\n",
    "\n",
    "plt.title('ARIMA(1,2,1) Rolling Forecast Origin On Test Set')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ARIMA Prediction's Reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert base arima to monthly inflation rate\n",
    "merge_with_1_month_pct_change(base_arima_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert best arima to monthly inflation rate\n",
    "\n",
    "merge_with_1_month_pct_change(best_arima_test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
